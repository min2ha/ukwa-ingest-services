version: "3.2"

services:
  # ----------------------------------------------
  # UKWA Heritrix
  # ----------------------------------------------
  heritrix-worker:
    image: ukwa/heritrix-worker:2.2.0
    hostname: "h3-{{.Task.Slot}}"
    environment:
      - "JAVA_OPTS=-Xmx4g"
      - "MAX_TOE_THREADS=100"
      - "CLAMD_HOST=clamd"
      - "CLAMD_ENABLED=true"
      - "KAFKA_BOOTSTRAP_SERVERS=${EXTERNAL_HOSTNAME}:9094"
      - "KAFKA_CRAWLED_TOPIC=uris.crawled.dc"
      - "KAFKA_TOCRAWL_TOPIC=uris.tocrawl.dc"
      - "KAFKA_DISCARDED_FEED_ENABLED=false"
      - "KAFKA_DISCARDED_TOPIC=uris.discarded.dc"
      - "KAFKA_CONSUMER_ID={{.Task.Slot}}"
      - "KAFKA_CONSUMER_GROUP_SIZE=16" # NOTE this MUST be the same as the number of replicas below!
      - "KAFKA_SEEK_TO_BEGINNING=false" # For the Domain Crawl, we try to re-use the state folders, to cope with the size of the tocrawl feed.
      - "KAFKA_MAX_POLL_RECORDS=1000"
      - "KAFKA_NUM_MESSAGE_THREADS=16"
      - "RECORD_DECIDING_RULE=true"
      - "WRENDER_ENDPOINT=http://wrender:8010/render"
      - "WEBRENDER_ENABLED=false" # Don't render web-pages from within Heritrix for this crawl
      - "CDXSERVER_ENDPOINT=http://192.168.45.21:9090/dc-2018-1"
      - "CRAWL_NAME=domain"
      - "HERITRIX_USER=admin"
      - "HERITRIX_PASSWORD=bl_uk"
      - "LAUNCH_AUTOMATICALLY=false"
      - "PAUSE_AT_START=true"
      # Scope changes to make this a Domain Crawl:
      - "SURTS_SOURCE_FILE=domain-surts.txt" # Use the domain SURT scope
      - "GEOIP_GB_ENABLED=true" # Also allow GeoIP-based inclusion (GB is in scope)
      - "USER_AGENT_PREFIX=bl.uk_lddc_bot" # Also declare a different User Agent for this crawl.
      # Use hop depth to control the domain crawl:
      - "MAX_HOPS_DEFAULT=20"
    volumes:
      - /data/dc/heritrix/output:/heritrix/output
      - /data/dc/heritrix/state:/heritrix/state # Store state outside to avoid overloading the container - the hostname is used to keep different runs separate.
      - type: tmpfs
        target: /heritrix/scratch # Use RAM disk for scratch space.
    deploy:
      replicas: 16
    stop_grace_period: 2m # Give the H3 instances some time to shut down neatly following SIGTERM
    depends_on:
      - kafka-1
      - kafka-2
      - clamd

  # ----------------------------------------------
  # Clamd virus scanning service
  # ----------------------------------------------
  clamd:
    image: ukwa/clamd
    deploy:
      replicas: 16

  # ----------------------------------------------
  # Kafka: used for URI routing and logging
  # ----------------------------------------------
  # Needs a Zookeeper too
  # ----
  zookeeper:
    image: wurstmeister/zookeeper
    ports:
      - 2181:2181
    volumes:
      - /data/dc/kafka/zookeeper-data:/opt/zookeeper-3.4.9/data

  #
  # Kafka Broker Setup:
  #
  kafka-1:
    image: wurstmeister/kafka:1.1.0
    ports:
      - target: 9094
        published: 9094
        protocol: tcp
        mode: host
    depends_on:
      - zookeeper
    environment:
      HOSTNAME_COMMAND: "docker info | grep ^Name: | cut -d' ' -f 2"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://:9092,OUTSIDE://_{HOSTNAME_COMMAND}:9094
      KAFKA_LISTENERS: PLAINTEXT://:9092,OUTSIDE://:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR: 10
      KAFKA_LOG_RETENTION_HOURS: -1
      KAFKA_LOG_RETENTION_BYTES: -1
      KAFKA_NUM_PARTITIONS: 32
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      LOG4J_LOGGER_KAFKA: WARN
      # persistent files are in consistent locations, each server supplies a /kafka folder.
      KAFKA_BROKER_ID: 1
      KAFKA_LOG_DIRS: /kafka/kafka-logs/kafka-logs-broker-1
      # CREATE this topic on this node
      KAFKA_CREATE_TOPICS: "uris.tocrawl.dc:256:1 --config=compression.type=snappy"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /data/dc/kafka/kafka-logs-broker-1:/kafka/kafka-logs/kafka-logs-broker-1
    deploy:
      replicas: 1
      placement:
        constraints: [node.hostname == crawler04.bl.uk]

  # Second Kafka instance
  kafka-2:
    image: wurstmeister/kafka:1.1.0
    ports:
      - target: 9094
        published: 9094
        protocol: tcp
        mode: host
    depends_on:
      - zookeeper
    environment:
      HOSTNAME_COMMAND: "docker info | grep ^Name: | cut -d' ' -f 2"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://:9092,OUTSIDE://_{HOSTNAME_COMMAND}:9094
      KAFKA_LISTENERS: PLAINTEXT://:9092,OUTSIDE://:9094
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR: 10
      KAFKA_LOG_RETENTION_HOURS: -1
      KAFKA_LOG_RETENTION_BYTES: -1
      KAFKA_NUM_PARTITIONS: 32
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      LOG4J_LOGGER_KAFKA: WARN
      # persistent files are in consistent locations, each server supplies a /kafka folder.
      KAFKA_BROKER_ID: 2
      KAFKA_LOG_DIRS: /kafka/kafka-logs/kafka-logs-broker-2
      # CREATE this topic on this node
      KAFKA_CREATE_TOPICS: "uris.crawled.dc:64:1 --config=compression.type=snappy,uris.discarded.dc:256:1 --config=compression.type=snappy"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /data/dc/kafka/kafka-logs-broker-2:/kafka/kafka-logs/kafka-logs-broker-2
    deploy:
      replicas: 1
      placement:
        constraints: [node.hostname == crawler01.bl.uk]

  # Kafka UI
  kafka-ui:
    image: ukwa/docker-trifecta
    ports:
      - "9000:9000"
    environment:
      - "ZK_HOST=zookeeper:2181"
    depends_on:
      - zookeeper

  # ----------------------------------------------
  # Monitoring: Expose crawl metrics to Prometheus
  # ----------------------------------------------
  hapy-dash:
    image: ukwa/ukwa-manage
    ports:
      - 9118:8000
    environment:
      - "HERITRIX_USERNAME=admin"
      - "HERITRIX_PASSWORD=bl_uk"
      - "CRAWL_JOBS_FILE=/crawl-jobs.json"
    volumes:
      - /root/github/ukwa-ingest-services/deploy/dc/crawl-jobs.json:/crawl-jobs.json
    depends_on:
      - heritrix-worker

  # ----------------------------------------------
  # QA and mid-crawl playback
  # ----------------------------------------------

  # This resolves WARC files locally, by filename:
  warc-server:
    image: ukwa/warc-server
    ports:
      - 8001:8000
    environment:
      - "WARC_PATHS=/heritrix/output,/heritrix/wren"
    volumes:
      - /data/dc/heritrix/output:/heritrix/output
      - /data/dc/heritrix/wren:/heritrix/wren

  # OpenWayback instance:
  openwayback:
    image: ukwa/waybacks
    ports:
      - "8080:8080"
      - "8090:8090"
    environment:
      - "UKWA_OWB_VERSION=qa"
      - "WAYBACK_URL_PORT=8080"
      - "WAYBACK_PROXY_PORT=8090"
      - "CDX_WHITELIST="
      - "WAYBACK_EXCLUDE_FILE=/usr/local/tomcat/webapps/ROOT/WEB-INF/classes/exclude.txt"
      - "WAYBACK_EMBARGO=0"
      - "WAYBACK_HTTPFS_PREFIX=http://warc-server:8000/by-filename/"
      - "CDX_INDEX_SERVER=http://192.168.45.21:9090/dc-2018-1"
      - "WAYBACK_URL_PREFIX=http://${EXTERNAL_HOSTNAME}:8080"
      - "WAYBACK_URL_HOST=${EXTERNAL_HOSTNAME}"

  # Python Wayback
  pywb:
    image: ukwa/ukwa-pywb
    ports:
      - "8081:8080"
    environment:
      - "WEBHDFS_USER=access"
      - "UKWA_INDEX=xmlquery+http://192.168.45.21:9090/dc-2018-1"
      - "UKWA_ARCHIVE=webhdfs://warc-server:8000/by-filename/"

